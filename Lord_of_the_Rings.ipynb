{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l53b03152Hh7",
        "outputId": "177c8ee1-d6e2-45c2-be6f-4fb717e69317"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from googleapiclient.discovery import build\n",
        "from textblob import TextBlob\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC7-cFzTjnuk"
      },
      "source": [
        "# Download Comments (Do not run again unless you have a google API key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-VbVYLsnyFE"
      },
      "source": [
        "Fetch YouTube Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNqiPm_f2ozf",
        "outputId": "4aafd770-acd8-4e7f-ab5a-bf538174b6ac"
      },
      "outputs": [],
      "source": [
        "youtube = build('youtube', 'v3', developerKey='KEY')\n",
        "\n",
        "video_id = '78IJdhvY1zg'\n",
        "comments = []\n",
        "\n",
        "request = youtube.commentThreads().list(\n",
        "    part='snippet',\n",
        "    videoId=video_id,\n",
        "    textFormat='plainText',\n",
        ")\n",
        "\n",
        "while request:\n",
        "    response = request.execute()\n",
        "    for item in response['items']:\n",
        "        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "        comments.append(comment)\n",
        "\n",
        "    request = youtube.commentThreads().list_next(request, response)\n",
        "\n",
        "for comment in comments:\n",
        "    print(comment)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsxmGPMCn4Xb"
      },
      "source": [
        "Pre-label Comments and Save to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41O4A7P9bagS"
      },
      "outputs": [],
      "source": [
        "def analyze_sentiment(comment):\n",
        "    analysis = TextBlob(comment)\n",
        "\n",
        "    if analysis.sentiment.polarity > 0:\n",
        "        return 'positive'\n",
        "    elif analysis.sentiment.polarity < 0:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "labeled_comments = []\n",
        "\n",
        "for comment in comments:\n",
        "    sentiment_label = analyze_sentiment(comment)\n",
        "    labeled_comments.append({\n",
        "        'comment': comment,\n",
        "        'sentiment': sentiment_label\n",
        "    })\n",
        "\n",
        "\n",
        "csv_filename = 'labeled_comments.csv'\n",
        "\n",
        "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    fieldnames = ['comment', 'sentiment']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    writer.writerows(labeled_comments)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pyBgFwjjqrA"
      },
      "source": [
        "# Train Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwzdPOadn_x7"
      },
      "source": [
        "Load Data from Excel File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RahbDNweCc4f",
        "outputId": "25997692-6e36-4e5d-80cc-dbf8a5ffd242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label in data: positive, Frequency: 631\n",
            "Label in data: neutral, Frequency: 510\n",
            "Label in data: negative, Frequency: 434\n"
          ]
        }
      ],
      "source": [
        "file_path = '/content/Data.xlsx'\n",
        "data = pd.DataFrame(pd.read_excel(file_path))\n",
        "\n",
        "for label, count in Counter(data['sentiment']).items():\n",
        "    print(f\"Label in data: {label}, Frequency: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1J63PX-ktEZ"
      },
      "source": [
        "Select a Balanced Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_udWmK-Ug4fu",
        "outputId": "c9258b9e-25b6-488c-817d-f78179d3fa35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1302\n"
          ]
        }
      ],
      "source": [
        "min_frequency = min(Counter(data['sentiment']).values())\n",
        "data = pd.concat([data[data['sentiment'] == label].sample(min_frequency) for label in Counter(data['sentiment']).keys()])\n",
        "print(len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78_GXMfQoKs3"
      },
      "source": [
        "Preprocess Data - Tokenization and POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "q378PCMiB9VN"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_analyze(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = [tag[1] for tag in pos_tag(tokens)]\n",
        "    return tokens, pos_tags\n",
        "\n",
        "data['tokens'], data['pos_tags'] = zip(*data['comment'].apply(preprocess_and_analyze).tolist())\n",
        "data.to_csv('preprocessed_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOws2CW3ofXi"
      },
      "source": [
        "Sentiment Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "FLq5qCIBoiJY"
      },
      "outputs": [],
      "source": [
        "sentiment_mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
        "label_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "data['sentiment_numeric'] = data['sentiment'].map(sentiment_mapping)\n",
        "sentiment_classes = ['negative', 'neutral', 'positive']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctcL1a8XojQV"
      },
      "source": [
        "Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "_ywVYA3kokot"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data['comment'], data['sentiment_numeric'], test_size=0.2, random_state=42,\n",
        "                                                    stratify=data['sentiment_numeric'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrVLofJNorBS"
      },
      "source": [
        "TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3KPnooUZwRI",
        "outputId": "29851d63-5b80-45f2-8f21-91d5b6758501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Words in Data:  2827\n"
          ]
        }
      ],
      "source": [
        "ngram_range = (1, 3)\n",
        "num = data['comment'].str.split().explode().nunique()\n",
        "print('Unique Words in Data: ', num)\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=ngram_range, min_df=5, max_features=num, stop_words='english')\n",
        "tfidf_vectorizer_for_nb = TfidfVectorizer(ngram_range=ngram_range, min_df=5, max_features=num, stop_words='english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnyeU6-Doxlr"
      },
      "source": [
        "Print Number of Labels in Train & Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBUm51lSo0xR",
        "outputId": "1aae2728-10d9-4289-e34f-b148fd49dc85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label in train: neutral, Frequency: 347\n",
            "Label in train: negative, Frequency: 347\n",
            "Label in train: positive, Frequency: 347\n",
            "Label in test: positive, Frequency: 87\n",
            "Label in test: negative, Frequency: 87\n",
            "Label in test: neutral, Frequency: 87\n"
          ]
        }
      ],
      "source": [
        "for label_train, count_train in Counter(y_train).items():\n",
        "    label_train = label_mapping[label_train]\n",
        "    print(f'Label in train: {label_train}, Frequency: {count_train}')\n",
        "\n",
        "for label_test, count_test in Counter(y_test).items():\n",
        "    label_test = label_mapping[label_test]\n",
        "    print(f'Label in test: {label_test}, Frequency: {count_test}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBXQGgjevsyT"
      },
      "source": [
        "Train-Test Split with Tokens and POS Taggins Information for NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "f1mmeSjZvobM"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame()\n",
        "df['combined_text'] = data['comment'].astype(str) + data['tokens'].astype(str) + data['pos_tags'].astype(str)\n",
        "matrix_tfidf = tfidf_vectorizer_for_nb.fit_transform(df['combined_text'])\n",
        "data_tfidf = pd.DataFrame(matrix_tfidf.toarray(), columns=tfidf_vectorizer_for_nb.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPv4LWpio3TW"
      },
      "source": [
        "Naive Bayes Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNbKy5_dGpd5",
        "outputId": "68eb6162-6fc8-435b-b5ad-7d9d8397e7cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.78\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.82      0.83      0.82        87\n",
            "     neutral       0.74      0.72      0.73        87\n",
            "    negative       0.77      0.78      0.78        87\n",
            "\n",
            "    accuracy                           0.78       261\n",
            "   macro avg       0.78      0.78      0.78       261\n",
            "weighted avg       0.78      0.78      0.78       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train_nb, X_test_nb, y_train_nb, y_test_nb = train_test_split(\n",
        "    data_tfidf, data['sentiment_numeric'], test_size=0.2, random_state=67, stratify=data['sentiment_numeric'])\n",
        "\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_nb, y_train_nb)\n",
        "y_pred = nb_classifier.predict(X_test_nb)\n",
        "accuracy = accuracy_score(y_test_nb, y_pred)\n",
        "nb_classification_rep = classification_report(y_test_nb, y_pred, target_names=sentiment_mapping.keys())\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print(nb_classification_rep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR-s5AwMo-wv"
      },
      "source": [
        "SVM Classification with Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F19l72g_gNfB",
        "outputId": "8cefd489-4e85-4a79-c19f-7681237d8dcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Grid Search Results:\n",
            "                                              params  mean_test_score  \\\n",
            "0   {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}         0.688774   \n",
            "1      {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}         0.642524   \n",
            "2    {'C': 0.1, 'gamma': 'auto', 'kernel': 'linear'}         0.688774   \n",
            "3       {'C': 0.1, 'gamma': 'auto', 'kernel': 'rbf'}         0.460259   \n",
            "4     {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}         0.783842   \n",
            "5        {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}         0.851113   \n",
            "6      {'C': 1, 'gamma': 'auto', 'kernel': 'linear'}         0.783842   \n",
            "7         {'C': 1, 'gamma': 'auto', 'kernel': 'rbf'}         0.460259   \n",
            "8    {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}         0.818458   \n",
            "9       {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}         0.860710   \n",
            "10    {'C': 10, 'gamma': 'auto', 'kernel': 'linear'}         0.818458   \n",
            "11       {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}         0.598486   \n",
            "12  {'C': 100, 'gamma': 'scale', 'kernel': 'linear'}         0.828073   \n",
            "13     {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}         0.860710   \n",
            "14   {'C': 100, 'gamma': 'auto', 'kernel': 'linear'}         0.828073   \n",
            "15      {'C': 100, 'gamma': 'auto', 'kernel': 'rbf'}         0.773293   \n",
            "\n",
            "    rank_test_score  \n",
            "0                11  \n",
            "1                13  \n",
            "2                11  \n",
            "3                15  \n",
            "4                 8  \n",
            "5                 3  \n",
            "6                 8  \n",
            "7                15  \n",
            "8                 6  \n",
            "9                 1  \n",
            "10                6  \n",
            "11               14  \n",
            "12                4  \n",
            "13                1  \n",
            "14                4  \n",
            "15               10  \n",
            "Best Estimator:  SVC(C=10, random_state=42)\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.88      0.87      0.88        87\n",
            "     neutral       0.81      0.94      0.87        87\n",
            "    negative       0.92      0.78      0.84        87\n",
            "\n",
            "    accuracy                           0.87       261\n",
            "   macro avg       0.87      0.87      0.87       261\n",
            "weighted avg       0.87      0.87      0.87       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10,100],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "svm_model = SVC(random_state=42)\n",
        "svm_model_grid = GridSearchCV(estimator=svm_model, param_grid=param_grid_svm, verbose=10, cv=5, n_jobs=-1)\n",
        "svm_model_grid.fit(X_train_tfidf, y_train)\n",
        "\n",
        "results_df = pd.DataFrame(svm_model_grid.cv_results_)\n",
        "print('Grid Search Results:')\n",
        "print(results_df[['params', 'mean_test_score', 'rank_test_score']])\n",
        "\n",
        "best_estimator_svm = svm_model_grid.best_estimator_\n",
        "print('Best Estimator: ', best_estimator_svm)\n",
        "\n",
        "y_pred = best_estimator_svm.predict(X_test_tfidf)\n",
        "\n",
        "svm_report = classification_report(y_test, y_pred, target_names=sentiment_mapping.keys())\n",
        "\n",
        "print('Classification Report:')\n",
        "print(svm_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn5rUksOpFbl"
      },
      "source": [
        "LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX4DySdyD2nX",
        "outputId": "37146484-e60b-4617-c278-d514fb598d8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "59/59 [==============================] - 11s 91ms/step - loss: 1.1011 - accuracy: 0.3056 - val_loss: 1.1029 - val_accuracy: 0.3048\n",
            "Epoch 2/10\n",
            "59/59 [==============================] - 3s 47ms/step - loss: 1.0998 - accuracy: 0.3216 - val_loss: 1.0997 - val_accuracy: 0.3048\n",
            "Epoch 3/10\n",
            "59/59 [==============================] - 3s 47ms/step - loss: 1.1003 - accuracy: 0.3120 - val_loss: 1.1028 - val_accuracy: 0.3048\n",
            "Epoch 4/10\n",
            "59/59 [==============================] - 3s 48ms/step - loss: 1.0999 - accuracy: 0.3098 - val_loss: 1.1010 - val_accuracy: 0.3048\n",
            "Epoch 5/10\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 1.0995 - accuracy: 0.3280 - val_loss: 1.1005 - val_accuracy: 0.3048\n",
            "Epoch 6/10\n",
            "59/59 [==============================] - 3s 50ms/step - loss: 1.0992 - accuracy: 0.3323 - val_loss: 1.1006 - val_accuracy: 0.3048\n",
            "Epoch 7/10\n",
            "59/59 [==============================] - 3s 46ms/step - loss: 1.0995 - accuracy: 0.3226 - val_loss: 1.1005 - val_accuracy: 0.3048\n",
            "Epoch 8/10\n",
            "59/59 [==============================] - 3s 47ms/step - loss: 1.0985 - accuracy: 0.3333 - val_loss: 1.1007 - val_accuracy: 0.3048\n",
            "Epoch 9/10\n",
            "59/59 [==============================] - 3s 49ms/step - loss: 1.0984 - accuracy: 0.3333 - val_loss: 1.1008 - val_accuracy: 0.2952\n",
            "Epoch 10/10\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 1.0976 - accuracy: 0.3141 - val_loss: 1.0992 - val_accuracy: 0.3048\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 389, 50)           19450     \n",
            "                                                                 \n",
            " bidirectional_12 (Bidirect  (None, 389, 100)          40400     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 389, 100)          0         \n",
            "                                                                 \n",
            " bidirectional_13 (Bidirect  (None, 100)               60400     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120553 (470.91 KB)\n",
            "Trainable params: 120553 (470.91 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "9/9 [==============================] - 1s 19ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.00      0.00      0.00        87\n",
            "     neutral       0.00      0.00      0.00        87\n",
            "    negative       0.33      1.00      0.50        87\n",
            "\n",
            "    accuracy                           0.33       261\n",
            "   macro avg       0.11      0.33      0.17       261\n",
            "weighted avg       0.11      0.33      0.17       261\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "model_lstm = Sequential()\n",
        "max_sequence_length = max(X_train_tfidf.shape[1], X_test_tfidf.shape[1])\n",
        "model_lstm.add(Embedding(input_dim=X_train_tfidf.shape[1], output_dim=50, input_length=max_sequence_length))\n",
        "model_lstm.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
        "model_lstm.add(Dropout(0.2))\n",
        "model_lstm.add(Bidirectional(LSTM(50)))\n",
        "model_lstm.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.fit(X_train_tfidf.toarray(), y_train, epochs=10, batch_size=16, validation_split = 0.1)\n",
        "\n",
        "model_lstm.summary()\n",
        "\n",
        "predictions = model_lstm.predict(X_test_tfidf.toarray())\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "predicted_sentiments = [label_mapping[label] for label in predicted_labels]\n",
        "lstm_tfidf_report = classification_report(y_test, predicted_labels, target_names=sentiment_mapping.keys())\n",
        "print(lstm_tfidf_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgL-LKbR3TZc"
      },
      "source": [
        "LSTM Using Pre-trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0pEJTnp3WOZ",
        "outputId": "5d4a539e-8ed9-4235-c397-f77f33453f57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 2827, 50)          102300    \n",
            "                                                                 \n",
            " bidirectional_14 (Bidirect  (None, 2827, 100)         40400     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 2827, 100)         0         \n",
            "                                                                 \n",
            " bidirectional_15 (Bidirect  (None, 100)               60400     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 203403 (794.54 KB)\n",
            "Trainable params: 101103 (394.93 KB)\n",
            "Non-trainable params: 102300 (399.61 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "59/59 [==============================] - 25s 295ms/step - loss: 0.9619 - accuracy: 0.5299 - val_loss: 0.8920 - val_accuracy: 0.6571\n",
            "Epoch 2/10\n",
            "59/59 [==============================] - 18s 299ms/step - loss: 0.7904 - accuracy: 0.6645 - val_loss: 0.6509 - val_accuracy: 0.7238\n",
            "Epoch 3/10\n",
            "59/59 [==============================] - 16s 271ms/step - loss: 0.6790 - accuracy: 0.7147 - val_loss: 0.5723 - val_accuracy: 0.7619\n",
            "Epoch 4/10\n",
            "59/59 [==============================] - 17s 284ms/step - loss: 0.6239 - accuracy: 0.7382 - val_loss: 0.4800 - val_accuracy: 0.8000\n",
            "Epoch 5/10\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.5208 - accuracy: 0.7885 - val_loss: 0.5297 - val_accuracy: 0.8476\n",
            "Epoch 6/10\n",
            "59/59 [==============================] - 16s 272ms/step - loss: 0.4409 - accuracy: 0.8355 - val_loss: 0.4389 - val_accuracy: 0.8571\n",
            "Epoch 7/10\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3672 - accuracy: 0.8526 - val_loss: 0.3653 - val_accuracy: 0.8571\n",
            "Epoch 8/10\n",
            "59/59 [==============================] - 16s 272ms/step - loss: 0.3411 - accuracy: 0.8750 - val_loss: 0.3556 - val_accuracy: 0.8857\n",
            "Epoch 9/10\n",
            "59/59 [==============================] - 16s 271ms/step - loss: 0.2725 - accuracy: 0.9017 - val_loss: 0.3641 - val_accuracy: 0.8857\n",
            "Epoch 10/10\n",
            "59/59 [==============================] - 16s 273ms/step - loss: 0.2255 - accuracy: 0.9124 - val_loss: 0.3315 - val_accuracy: 0.8476\n",
            "9/9 [==============================] - 2s 116ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.89      0.91      0.90        87\n",
            "     neutral       0.76      0.89      0.82        87\n",
            "    negative       0.90      0.74      0.81        87\n",
            "\n",
            "    accuracy                           0.84       261\n",
            "   macro avg       0.85      0.84      0.84       261\n",
            "weighted avg       0.85      0.84      0.84       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.astype(str)\n",
        "X_test = X_test.astype(str)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "tokenizer.fit_on_texts(X_test)\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_sequence_length = num\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "embedding_dim = 50\n",
        "embedding_matrix = {}\n",
        "\n",
        "with open('glove.6B.50d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_matrix[word] = coefs\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_matrix_for_model = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embedding_matrix.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_for_model[i] = embedding_vector\n",
        "\n",
        "model_pretrained = Sequential()\n",
        "model_pretrained.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix_for_model], input_length=max_sequence_length, trainable=False))\n",
        "model_pretrained.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
        "model_pretrained.add(Dropout(0.2))\n",
        "model_pretrained.add(Bidirectional(LSTM(50)))\n",
        "model_pretrained.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model_pretrained.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_pretrained.summary()\n",
        "model_pretrained.fit(X_train_padded, y_train, epochs=10, batch_size=16, validation_split=0.1)\n",
        "\n",
        "result = model_pretrained.predict(X_test_padded)\n",
        "predicted_labels = np.argmax(result, axis=1)\n",
        "predicted_sentiments = [label_mapping[label] for label in predicted_labels]\n",
        "lstm_pretrained_report = classification_report(y_test, predicted_labels, target_names=sentiment_mapping.keys())\n",
        "print(lstm_pretrained_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owKuOufbfUi0"
      },
      "source": [
        "Save Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "YOlZ7dACfTzv"
      },
      "outputs": [],
      "source": [
        "def save_model(variable, filename):\n",
        "    pickle.dump(variable, open(filename, \"wb\"))\n",
        "\n",
        "save_model(nb_classifier, 'Naive_Bayes.pickle')\n",
        "save_model(best_estimator_svm, 'SVM.pickle')\n",
        "save_model(model_lstm, 'LSTM_TF-IDF.pickle')\n",
        "save_model(model_pretrained, 'LSTM_Word2Vec.pickle')\n",
        "save_model(tfidf_vectorizer_for_nb, 'TF-IDF_for_NB.pickle')\n",
        "save_model(tfidf_vectorizer, 'TF-IDF.pickle')\n",
        "save_model(tokenizer, 'Word2Vec.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLczkkvsjg9p"
      },
      "source": [
        "# Load Model and Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4iwtg0pfcwt"
      },
      "source": [
        "Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "a3o1k06ifd32"
      },
      "outputs": [],
      "source": [
        "def load_model(filename):\n",
        "    variable = pickle.load(open(filename, \"rb\"))\n",
        "    return variable\n",
        "\n",
        "nb_classifier = load_model('Naive_Bayes.pickle')\n",
        "best_estimator_svm = load_model('SVM.pickle')\n",
        "moedel_lstm = load_model('LSTM_TF-IDF.pickle')\n",
        "model_pretrained = load_model('LSTM_Word2Vec.pickle')\n",
        "tfidf_vectorizer_for_nb = load_model('TF-IDF_for_NB.pickle')\n",
        "tfidf_vectorizer = load_model('TF-IDF.pickle')\n",
        "tokenizer = load_model('Word2Vec.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxWY0f3Rpqx_"
      },
      "source": [
        "Predict Sentiment for a Comment using All Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kyWORhqeJGu",
        "outputId": "16f843ea-ccfa-4e66-bf3f-b1102235426e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your comment: I love the movie\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "Tokens: ['I', 'love', 'the', 'movie']\n",
            "POS Tags: ['PRP', 'VBP', 'DT', 'NN']\n",
            "SVM Model Prediction: negative\n",
            "Naive Bayes Model Prediction: negative\n",
            "LSTM Model Prediction: positive\n",
            "LSTM Pretrained Model Prediction: positive\n"
          ]
        }
      ],
      "source": [
        "def predict_sentiment(comment):\n",
        "\n",
        "    tokens, pos_tags = preprocess_and_analyze(comment)\n",
        "\n",
        "    comment_vectorized = tfidf_vectorizer.transform([comment])\n",
        "    svm_prediction = best_estimator_svm.predict(comment_vectorized)\n",
        "\n",
        "    comment_combined = tfidf_vectorizer_for_nb.transform([' '.join(map(str, [comment] + tokens + pos_tags))])\n",
        "    comment_vectorized_combined = pd.DataFrame(comment_combined.toarray(), columns=tfidf_vectorizer_for_nb.get_feature_names_out())\n",
        "    nb_prediction = nb_classifier.predict(comment_vectorized_combined)\n",
        "\n",
        "    comment_sequence = tokenizer.texts_to_sequences([comment])\n",
        "    comment_padded = pad_sequences(comment_sequence, maxlen=2827, padding='post')\n",
        "\n",
        "    lstm_predictions = model_lstm.predict(comment_vectorized)\n",
        "    lstm_pretrained_prediction = model_pretrained.predict(comment_padded)\n",
        "\n",
        "    svm_predicted_class_index = np.argmax(svm_prediction)\n",
        "    nb_predicted_class_index = np.argmax(nb_prediction)\n",
        "    lstm_predicted_class_index = np.argmax(lstm_predictions)\n",
        "    lstm_pretrained_prediction_class_index = np.argmax(lstm_pretrained_prediction)\n",
        "\n",
        "    svm_sentiment = sentiment_classes[svm_predicted_class_index]\n",
        "    nb_sentiment = sentiment_classes[nb_predicted_class_index]\n",
        "    lstm_sentiment = sentiment_classes[lstm_predicted_class_index]\n",
        "    lstm_pretrained_sentiment = sentiment_classes[lstm_pretrained_prediction_class_index]\n",
        "\n",
        "    return svm_sentiment, nb_sentiment, lstm_sentiment, lstm_pretrained_sentiment, tokens, pos_tags\n",
        "\n",
        "\n",
        "def predict_from_command_line():\n",
        "    comment = input('Enter your comment: ')\n",
        "    if comment:\n",
        "        svm_sentiment, nb_sentiment, lstm_sentiment, lstm_pretrained_sentiment, tokens, pos_tags= predict_sentiment(comment)\n",
        "        print(f'Tokens: {tokens}')\n",
        "        print(f'POS Tags: {pos_tags}')\n",
        "        print(f'SVM Model Prediction: {svm_sentiment}')\n",
        "        print(f'Naive Bayes Model Prediction: {nb_sentiment}')\n",
        "        print(f'LSTM Model Prediction: {lstm_sentiment}')\n",
        "        print(f'LSTM Pretrained Model Prediction: {lstm_pretrained_sentiment}')\n",
        "    else:\n",
        "        print('Please enter a comment.')\n",
        "\n",
        "predict_from_command_line()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
